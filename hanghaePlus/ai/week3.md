# 항해 플러스 AI 3주차 후기 - 자연어 처리 끝판왕 GPT와 BERT, 제대로 파헤치기

이번 주차에는 항해플러스 AI 코스를 통해 자연어 처리(NLP)의 심화된 내용을 탐험하게 되었습니다. 지난 주 RNN과 Transformer를 통해 자연어 처리의 기본 원리를 이해했다면, 이번 주에는 Transfer Learning을 보다 깊게 배우고 BERT, DistilBERT, 그리고 GPT 모델을 실제로 활용하며 더 많은 깨달음을 얻을 수 있었습니다.

먼저, Transfer Learning에 대해 자세히 배우게 되었는데요. 처음에 Transfer Learning이라는 개념을 접했을 때는 지난주 학습을 통해 막연하게나마 이해하고 있었지만, 이번에 더 구체적으로 알게 되었습니다. Transfer Learning은 데이터가 충분한 문제에서 학습된 모델을 가져와 데이터가 부족한 다른 문제에 적용하는 방식입니다. 특히 자연어 처리는 모든 입력 데이터가 자연어로 통일되어 있기 때문에 이 방법이 매우 효과적이었습니다.

Transfer Learning은 크게 두 단계로 나뉩니다. 첫 번째는 Pre-training(사전학습) 단계인데요. 이 단계에서는 두 가지 방식이 있습니다. Supervised 방식과 Unsupervised 방식이죠. Supervised 방식은 정답이 있는 데이터를 사용해 학습하고, Unsupervised 방식은 정답이 없는 데이터를 통해 모델이 스스로 데이터의 구조와 패턴을 학습합니다. 특히 Unsupervised 방식이 충분히 강력한 모델을 만들어낼 수 있다는 점에서 놀라웠습니다.

이어서 Fine-tuning(미세조정) 단계에서는 사전 학습된 모델의 파라미터를 활용하여 실제 문제에 맞게 추가적으로 학습시키는 방법을 배웠습니다. Fine-tuning에도 모든 파라미터를 재학습하는 방식과 특정 층(layer)만 고정하고 학습하는 방식이 있습니다. 개인적으로는 과적합(overfitting)과 메모리 문제를 방지하기 위해 특정 층을 고정하는 방법이 효율적이라는 점이 매우 공감되었습니다.

이러한 개념을 실제로 적용하면서 BERT라는 모델에 대해 깊게 공부하게 되었습니다. BERT는 Unsupervised 방식으로 위키피디아 같은 일반 텍스트를 가지고 학습하며, 특히 Masked Language Model(MLM)과 Next Sentence Prediction(NSP)이라는 두 가지 방식을 사용하여 자연어의 이해 능력을 높였습니다. 특히 MLM은 문장에서 일부 단어를 마스킹하여 이를 모델이 맞추도록 하는 방식으로, 언어를 효과적으로 학습할 수 있었습니다. NSP는 두 문장이 실제로 연결된 문장인지 여부를 모델이 학습하도록 만들어 더욱 정교한 문맥 파악이 가능했습니다.

BERT에서 사용된 Masked Language Model(MLM)에 대해 좀 더 자세히 설명하면, MLM은 문장 내의 특정 단어들을 랜덤하게 마스킹하고, 그 마스킹된 부분을 예측하도록 모델을 훈련시키는 방식입니다. 이렇게 하면 모델은 문장의 전후 문맥을 활용하여 단어를 맞추기 때문에 자연어의 맥락과 의미를 보다 깊이 있게 학습할 수 있습니다.

이후 BERT를 더욱 가볍고 효율적으로 만든 DistilBERT에 대해서도 배웠습니다. DistilBERT는 Knowledge Distillation(지식 증류)이라는 방식을 통해 BERT의 성능은 유지하면서도 모델 크기를 줄이고 연산 속도를 높였습니다. Knowledge Distillation이란 큰 모델(Teacher 모델)의 지식을 작은 모델(Student 모델)에 전달하는 방법으로, Teacher 모델이 예측한 결과를 직접적으로 가르쳐서 Student 모델이 이를 빠르게 학습할 수 있도록 합니다. 이러한 방식 덕분에 실제 서비스 적용 시 자원 효율성과 성능을 모두 잡을 수 있습니다.

실제로 과제에서도 이 DistilBERT를 활용하여 뉴스 기사 분류 문제를 수행했는데요. Huggingface에서 제공하는 fancyzhx/ag_news 데이터셋을 사용하여 분류 문제를 해결했습니다. 특히 기존의 이진 분류가 아닌 다중 클래스 분류였기 때문에 모델의 출력 층을 조정하고 CrossEntropyLoss를 사용하여 문제를 풀었습니다.

과제를 수행하면서 각 epoch마다 loss를 확인하고, 최종적으로 정확도(accuracy)를 측정하여 성능을 평가했는데요. 실습을 통해 실제로 DistilBERT가 빠른 시간 내에 높은 정확도로 뉴스 기사들을 정확하게 분류하는 것을 확인하면서, 현실적인 서비스에 적용할 수 있겠다는 확신이 생겼습니다.

마지막으로 이번 주에는 GPT(Generative Pre-trained Transformer)에 대해서도 학습했습니다. GPT는 특히 텍스트 생성에 강력한 성능을 보이는 모델로, 주어진 텍스트의 다음 단어를 예측하는 Next Token Prediction 방식으로 학습됩니다. GPT는 BERT와 달리 텍스트를 실제로 생성할 수 있으며, 이러한 특성을 통해 기계 번역, 질의응답, 수학 연산 등 다양한 문제를 별도의 Fine-tuning 없이도 해결할 수 있었습니다. 특히 Few-shot Learning을 통해 몇 가지 예시만 제공하면 모델이 원하는 작업을 수행할 수 있다는 점에서 놀라웠습니다.

GPT 모델의 Few-shot Learning에 대해 추가로 설명하자면, Few-shot Learning은 매우 적은 수의 예시 데이터만으로 모델이 새로운 작업을 학습할 수 있게 해줍니다. 이는 모델이 미리 많은 양의 데이터로 학습된 덕분에 가능한 것이며, 인간이 소수의 예시만으로도 빠르게 학습하는 방식과 매우 유사하여 인상 깊었습니다. 또한 GPT-3와 같은 대형 모델은 방대한 데이터와 계산 자원을 통해 인간의 일반적인 언어 사용 능력을 거의 유사하게 구현하는 수준에 도달하여 AI 기술의 무한한 가능성을 보여주었습니다.

이번 주차를 통해 NLP 모델들을 실제로 구현하고 과제를 수행하면서, 점점 더 실무적인 능력을 갖추고 있음을 느낄 수 있었습니다. 특히 현실적인 문제들을 해결할 수 있는 모델을 직접 구현해보면서 앞으로의 학습과 발전이 더욱 기대가 됩니다.

다음 주차에도 항해플러스 AI 코스에서의 배움을 꾸준히 실천하며 성장해 나가겠습니다. 긴 글 읽어주셔서 감사합니다!

항해 플러스 — 추천인 코드: CF7LUQ

#항해99 #항해플러스AI후기 #AI개발자 #LLM